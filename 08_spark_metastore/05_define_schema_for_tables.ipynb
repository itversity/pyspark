{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Schema for Tables\n",
    "\n",
    "When we want to create a table using `spark.catalog.createTable` or using `spark.catalog.createExternalTable`, we need to specify Schema.\n",
    "\n",
    "* Schema can be inferred or we can pass schema using `StructType` object while creating the table..\n",
    "* `StructType` takes list of objects of type `StructField`.\n",
    "* `StructField` is built using column name and data type. All the data types are available under `pyspark.sql.types`.\n",
    "* We need to pass table name and schema for `spark.catalog.createTable`.\n",
    "* We have to pass path along with name and schema for `spark.catalog.createExternalTable`.\n",
    "* We can use source to define file format along with applicable options. For example, if we want to create a table for CSV, then source will be csv and we can pass applicable options for CSV such as sep, header etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform tasks to create empty table using `spark.catalog.createTable` or using `spark.catalog.createExternalTable`.\n",
    "\n",
    "* Create database **hr_db** and table **employees** with following fields. Let us create Database first and then we will see how to create table.\n",
    "  * employee_id of type Integer\n",
    "  * first_name of type String\n",
    "  * last_name of type String\n",
    "  * salary of type Float\n",
    "  * nationality of type String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_hr_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_hr_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build StructType object using StructField list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, \\\n",
    "    IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesSchema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"nationality\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DROP TABLE employees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create table by passing StructType object as schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.createTable(\"employees\", schema=employeesSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* List the tables from database created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listColumns('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DESCRIBE FORMATTED employees').show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create database by name **{username}_airlines** and create external table for **airport-codes.txt**.\n",
    "  * Data have header\n",
    "  * Fields in each record are delimited by a tab character.\n",
    "  * We can pass options such as sep, header, inferSchema etc to define the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.createExternalTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_airlines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To create external table, we need to have write permissions over the path which we want to use.\n",
    "* As we have only read permissions on **/public/airlines_all/airport-codes** we cannot use that path while creating external table.\n",
    "* Let us copy the data to **/user/`whoami`/airlines_all/airport-codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/`whoami`/airlines_all\n",
    "hdfs dfs -cp -f /public/airlines_all/airport-codes /user/`whoami`/airlines_all\n",
    "hdfs dfs -ls /user/`whoami`/airlines_all/airport-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -tail /user/`whoami`/airlines_all/airport-codes/airport-codes-na.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "airport_codes_path = f'/user/{username}/airlines_all/airport-codes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DROP TABLE airport_codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog. \\\n",
    "    createExternalTable(\"airport_codes\",\n",
    "                        path=airport_codes_path,\n",
    "                        source=\"csv\",\n",
    "                        sep=\"\\t\",\n",
    "                        header=\"true\",\n",
    "                        inferSchema=\"true\"\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table(\"airport_codes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DESCRIBE FORMATTED airport_codes').show(100, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
